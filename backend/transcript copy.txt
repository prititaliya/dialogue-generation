Speaker 1: For example, you have a live system running on your browser, and if there's a malicious attack that has been injected from somewhere, that.
Speaker 1: System detects it. What we try to do is we try to wrap those attacks by benign behavior and try to hide them.
Speaker 1: So we are doing the bad.
Speaker 2: Yeah, I get it.
Speaker 1: Yeah. Uh.
Speaker 1: I could give you an example by drawing how we basically hide the malicious attack.
Speaker 2: The other one.
Speaker 2: Is.
Speaker 1: For example, this is a graph.
Speaker 1: Of entities and edges. Uh, connecting them.
Speaker 1: It could be Python. Execute a text file, something like that. Let's take, for example. This is a malicious file that has been injected. We try to find for a benign file a benign entity in the whole graph that resembles this malicious.
Speaker 1: File. It could be the number of edges connected to the file, something that makes them similar to each other. For example, you find this file we try to mimic its interactions around the malicious node so that when we pass it through a machine learning model, the model thinks that because it is surrounded by benign behavior, the node must be benign as well.
Speaker 1: So that's.
Speaker 2: Works on machine learning.
Speaker 1: Yes.
Speaker 2: And so you're trying to get some input that mimics what goes in general. And then the machine learning model does not know whether this is known.
Speaker 1: The model is trained purely on benign behavior. When it sees something odd some odd behavior, it knows it's not benign. It's bad. So we try to connect benign behavior to the bad behavior. So we make it kind of we.
Speaker 1: Make it something that the model can recognize, something it has been trained on. It thinks so that it does not classify this as an attack. Is regular benign behavior for the model.
Speaker 1: How?
Speaker 2: Do.
Speaker 2: You train models to go into the.
Speaker 2: Know or something.
Speaker 1: The models. There are.
Speaker 2: I know you're trying to get something into the model. Do you know that it looks something similar to the other nodes or other input? How do you mimic the input?
Speaker 1: Okay.
Speaker 2: Step nine.
Speaker 1: Yeah. For example, this is the benign node. Yeah. How do we find the benign node is based on similarities to the malicious nodes. We see how similar a given benign node is to the malicious node. It can be based on the context the edges. It has.
Speaker 1: And then for example this benign node is connected to another benign node. This is the interaction of this node. We connect this interaction to the malicious node as well. So that when we.
Speaker 1: Surrounded by benign behavior the model thinks it's benign. That's how we try to hide it. That's what we do.
Speaker 1: You want to go more?
Speaker 1: Uh, more in depth.
Speaker 3: Think of it as, um, a in in.
Speaker 3: Like in normal behavior, for example, uh, when you open crew, that crew, um, opens a.
Speaker 3: DLL files. Files.
Speaker 3: They are just files that has codes in classes. Have you heard of classes like. An op? Yeah. Yeah. So HD.
Speaker 3: HD file.
Speaker 3: Has, uh.
Speaker 3: A class that is used for.
Speaker 3: I'm not pretty sure you can search for it. Um.
Speaker 3: It is something. Library. Something. It is just, uh, normal. Normal class that Google execute. Okay, so if one time, uh, that Google Chrome will be attack didn't execute these files.
Speaker 3: So it means something wrong happened. So it means that the attacker injected some malicious code. What we do is.
Speaker 3: We know the normal behavior of the group, which opens the DLL file. So we add these DLL files to the malicious system so that it seems.
Speaker 3: So. Next step is to train an system.
Speaker 3: Model. Where it is we have to design architecture some architecture so that it it it captures these types of.
Speaker 3: Okay.
Speaker 2: So, so that we don't do this manually.
Speaker 3: We don't do it because it's.
Speaker 3: Thousands.
Speaker 2: So how do you search for these? Uh.
Speaker 3: Using artificial intelligence or using, uh, it's going to be too much details. I cannot talk about it, but using artificial intelligence, you can, um, in the embedding space, you get.
Speaker 3: So embedding space, we represent each file with.
Speaker 3: Numbers. So by.
Speaker 3: Searching with these in these numbers, these embedding space, we can match.
Speaker 3: Uh which benign node is most similar to.
Speaker 2: Using machine learning.
Speaker 3: Using machine.
Speaker 2: Learning. And then you try to crack machine learning. Yeah.
Speaker 2: Yeah.
Speaker 3: So, so.
Speaker 3: Yeah. Machine learning is kind of like.
Speaker 2: It's a tool.
Speaker 3: It's a tool, and it's so big. It's. We.
Speaker 2: So what kind of model are you using? Are you using any pre-trained pre-trained model or,
Speaker 3: Uh, in.
Speaker 3: In intrusion detection systems. Uh, I.
Speaker 3: Can give you insights about what techniques exist. One technique is training variational autoencoder.
Speaker 3: That consists of the coder and encoder and the.
Speaker 3: Coder. So we.
Speaker 3: Teach it. We train the model to be able to encode benign behavior. Benign problems in graphs.
Speaker 3: And then decoded with very small reconstruction error.
Speaker 3: Once it is well trained, when a malicious behavior becomes an input to that model, the reconstruction error becomes so high, the model cannot reconstruct it because it's not trained on it previously. Right. So this is one thing from there, from the reconstruction error, when it's so high, we detect that there is a malicious behavior.
Speaker 3: This is one technique. Another technique could be graph neural network. If you do it and message.
Speaker 3: Technique. So basically um we train that model with the benign behavior to predict the label of each node.
Speaker 3: Okay. Once the embedding the list is assumed that the normal behavior embedding are all positives. Once a malicious behavior occurs, takes place, the embeddings are in negatives. For example. Okay, so now the yield in the graph neural network would not be able to predict the label of that negative feature. So now it would capture it would raise a flag. It would raise an alarm. So these two techniques that are used, there are many more techniques.
Speaker 2: Okay.

Stop command detected: Stop recording.
